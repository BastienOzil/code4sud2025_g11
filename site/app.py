"""
Application Flask pour l'analyse de march√© bas√©e sur data.gouv.fr
Projet Code4Sud - Sujet 2: Aide aux √©tudes de march√©
"""

from flask import Flask, render_template, request, jsonify
from flask_cors import CORS
import requests
import json
from datetime import datetime
import os
import ollama

app = Flask(__name__)
CORS(app)

# Configuration
DATA_GOUV_API = "https://www.data.gouv.fr/api/1"
CACHE_DIR = "cache"
OLLAMA_MODEL = "mistral"  # Mod√®le Ollama √† utiliser

# Cr√©er le dossier cache s'il n'existe pas
if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)


class DataGouvClient:
    """Client pour interagir avec l'API de data.gouv.fr"""
    
    def __init__(self):
        self.base_url = DATA_GOUV_API
        
    def search_datasets(self, query, page_size=20):
        """Rechercher des datasets sur data.gouv.fr"""
        try:
            url = f"{self.base_url}/datasets/"
            params = {
                'q': query,
                'page_size': page_size
            }
            response = requests.get(url, params=params, timeout=5)  # Timeout de 5 secondes
            response.raise_for_status()
            return response.json()
        except requests.Timeout:
            print(f"Timeout lors de la recherche: {query}")
            return {'data': []}
        except Exception as e:
            print(f"Erreur lors de la recherche: {e}")
            return {'data': []}
    
    def get_dataset(self, dataset_id):
        """R√©cup√©rer un dataset sp√©cifique"""
        try:
            url = f"{self.base_url}/datasets/{dataset_id}/"
            response = requests.get(url)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"Erreur lors de la r√©cup√©ration du dataset: {e}")
            return None
    
    def download_resource(self, resource_url):
        """T√©l√©charger une ressource (fichier CSV, JSON, etc.)"""
        try:
            response = requests.get(resource_url)
            response.raise_for_status()
            return response.content
        except Exception as e:
            print(f"Erreur lors du t√©l√©chargement: {e}")
            return None


class OllamaAIAnalyzer:
    """Analyseur IA utilisant Ollama pour l'analyse intelligente"""
    
    def __init__(self, model=OLLAMA_MODEL):
        self.model = model
        self.ollama_available = self._check_ollama()
    
    def _check_ollama(self):
        """V√©rifier si Ollama est disponible"""
        try:
            ollama.list()
            return True
        except Exception as e:
            print(f"Ollama non disponible: {e}")
            return False
    
    def analyze_datasets(self, datasets, sector, location=None):
        """Analyser les datasets avec l'IA - VERSION RAPIDE"""
        if not self.ollama_available or not datasets:
            return None
        
        # Pr√©parer un contexte COURT pour l'IA
        datasets_summary = "\n".join([
            f"- {ds.get('title', '')[:80]}"
            for ds in datasets[:3]  # Seulement 3 datasets au lieu de 5
        ])
        
        # Prompt COURT et DIRECT
        prompt = f"""Analyse rapide du march√© {sector}{f' √† {location}' if location else ''}.

Datasets: {datasets_summary}

En 3 points courts (max 150 mots):
1. Pertinence des donn√©es
2. Opportunit√© principale
3. Risque principal"""

        try:
            response = ollama.generate(
                model=self.model, 
                prompt=prompt,
                options={
                    'temperature': 0.7,
                    'num_predict': 200  # Limite √† 200 tokens pour acc√©l√©rer
                }
            )
            return response['response']
        except Exception as e:
            print(f"Erreur Ollama: {e}")
            return None
    
    def generate_recommendations(self, sector, location, datasets_count):
        """G√©n√©rer des recommandations personnalis√©es avec l'IA - VERSION RAPIDE"""
        if not self.ollama_available:
            return None
        
        # Prompt TR√àS COURT
        prompt = f"""3 conseils rapides pour √©tude de march√© {sector}{f' √† {location}' if location else ''}.

{datasets_count} datasets trouv√©s.

Format: liste num√©rot√©e, 1 ligne par conseil."""

        try:
            response = ollama.generate(
                model=self.model, 
                prompt=prompt,
                options={
                    'temperature': 0.7,
                    'num_predict': 150  # Limite stricte
                }
            )
            return response['response']
        except Exception as e:
            print(f"Erreur Ollama: {e}")
            return None
    
    def analyze_competition(self, sector, location=None):
        """Analyser la concurrence avec l'IA - D√âSACTIV√â PAR D√âFAUT POUR VITESSE"""
        # D√©sactiv√© pour acc√©l√©rer - peut √™tre r√©activ√© si besoin
        return None
        
        # Code original comment√© pour r√©f√©rence
        # if not self.ollama_available:
        #     return None
        # 
        # prompt = f"""Points cl√©s pour analyser la concurrence {sector}{f' √† {location}' if location else ''}.
        # 
        # 3 axes d'analyse principaux (tr√®s court)."""
        # 
        # try:
        #     response = ollama.generate(
        #         model=self.model, 
        #         prompt=prompt,
        #         options={'num_predict': 100}
        #     )
        #     return response['response']
        # except Exception as e:
        #     print(f"Erreur Ollama: {e}")
        #     return None
    
    def generate_generic_recommendations(self, sector, location=None):
        """G√©n√©rer des recommandations m√™me sans datasets"""
        if not self.ollama_available:
            return None
        
        prompt = f"""Conseils pour √©tude de march√© {sector}{f' √† {location}' if location else ''}.

Aucune donn√©e publique sp√©cifique trouv√©e.

3 conseils pour trouver des informations utiles (sources alternatives, approches cr√©atives).
Format: liste courte."""

        try:
            response = ollama.generate(
                model=self.model,
                prompt=prompt,
                options={
                    'temperature': 0.8,
                    'num_predict': 200
                }
            )
            return response['response']
        except Exception as e:
            print(f"Erreur Ollama: {e}")
            return None


class MarketAnalyzer:
    """Analyseur de march√© utilisant les donn√©es de data.gouv.fr"""
    
    def __init__(self):
        self.data_client = DataGouvClient()
        self.ai_analyzer = OllamaAIAnalyzer()
        
        # Mapping des secteurs vers des termes de recherche plus larges
        self.sector_mappings = {
            'boulangerie': ['commerce alimentaire', 'artisanat', 'commerces', '√©tablissements'],
            'restaurant': ['restauration', 'commerce', 'tourisme', '√©tablissements'],
            'commerce': ['commerces', '√©tablissements', 'entreprises', '√©conomie'],
            'technologie': ['innovation', 'num√©rique', 'entreprises', 'startups'],
            'sant√©': ['sant√©', '√©tablissements sant√©', 'professionnels sant√©'],
            'immobilier': ['logement', 'construction', 'foncier', 'urbanisme'],
            'tourisme': ['tourisme', 'h√©bergement', 'culture', 'loisirs'],
            'transport': ['transport', 'mobilit√©', 'infrastructure'],
            'agriculture': ['agriculture', 'exploitation agricole', 'alimentaire'],
            '√©ducation': ['√©ducation', 'formation', '√©tablissements scolaires'],
        }
    
    def _expand_search_terms(self, sector):
        """√âlargir les termes de recherche pour un secteur"""
        # Normaliser le secteur
        sector_lower = sector.lower()
        
        # Chercher des correspondances
        expanded_terms = [sector]
        
        for key, terms in self.sector_mappings.items():
            if key in sector_lower or sector_lower in key:
                expanded_terms.extend(terms)
                break
        
        # Ajouter des termes g√©n√©riques si rien trouv√©
        if len(expanded_terms) == 1:
            expanded_terms.extend(['entreprises', '√©tablissements', 'activit√© √©conomique'])
        
        return expanded_terms
    
    def _analyze_step1_market_size(self, datasets, sector, location):
        """√âTAPE 1: Analyser la taille du march√©"""
        analysis = {
            'title': '√âtape 1 - Taille du march√©',
            'data': {},
            'insights': []
        }
        
        # Compter les √©tablissements/entreprises dans les datasets
        total_resources = sum(len(ds.get('resources', [])) for ds in datasets)
        
        analysis['data'] = {
            'datasets_disponibles': len(datasets),
            'ressources_totales': total_resources,
            'secteur': sector,
            'zone': location or 'France'
        }
        
        analysis['insights'].append(f"{len(datasets)} sources de donn√©es identifi√©es pour {sector}")
        if location:
            analysis['insights'].append(f"Zone g√©ographique: {location}")
        analysis['insights'].append(f"{total_resources} fichiers de donn√©es disponibles")
        
        # Estimer la taille du march√©
        if total_resources > 50:
            analysis['insights'].append("March√© important avec de nombreuses donn√©es disponibles")
        elif total_resources > 20:
            analysis['insights'].append("March√© de taille moyenne, donn√©es suffisantes")
        else:
            analysis['insights'].append("March√© de niche, donn√©es limit√©es")
        
        return analysis
    
    def _analyze_step2_target_audience(self, datasets, sector, location):
        """√âTAPE 2: Identifier la client√®le cible"""
        analysis = {
            'title': '√âtape 2 - Client√®le cible',
            'data': {},
            'insights': []
        }
        
        # D√©terminer les personas en fonction du secteur
        personas = {
            'restaurant': ['Familles', 'Professionnels en pause d√©jeuner', 'Touristes', '√âtudiants'],
            'boulangerie': ['Habitants du quartier', 'Travailleurs locaux', 'Familles', 'Retrait√©s'],
            'commerce': ['Consommateurs locaux', 'Entreprises B2B', 'Touristes'],
            'technologie': ['Entreprises', 'Startups', 'Collectivit√©s', 'Particuliers tech-savvy'],
            'sant√©': ['Patients locaux', 'Personnes √¢g√©es', 'Familles avec enfants', 'Sportifs'],
            'tourisme': ['Touristes nationaux', 'Touristes internationaux', 'Excursionnistes', 'Groupes'],
        }
        
        # Identifier le type de client√®le
        sector_lower = sector.lower()
        target_personas = []
        
        for key, values in personas.items():
            if key in sector_lower or sector_lower in key:
                target_personas = values
                break
        
        if not target_personas:
            target_personas = ['Grand public', 'Professionnels', 'Collectivit√©s']
        
        analysis['data'] = {
            'personas_identifies': target_personas,
            'nombre_segments': len(target_personas)
        }
        
        analysis['insights'].append(f"{len(target_personas)} segments de client√®le identifi√©s:")
        for persona in target_personas:
            analysis['insights'].append(f"  ‚Ä¢ {persona}")
        
        if location:
            analysis['insights'].append(f"Ciblage g√©ographique: {location}")
        
        return analysis
    
    def _analyze_step3_competition(self, datasets, sector, location):
        """√âTAPE 3: Analyser la concurrence"""
        analysis = {
            'title': '√âtape 3 - Concurrence',
            'data': {},
            'insights': []
        }
        
        # Chercher des donn√©es sur les √©tablissements
        establishment_data = []
        for ds in datasets:
            title_lower = ds.get('title', '').lower()
            if any(word in title_lower for word in ['√©tablissement', 'entreprise', 'commerce', 'sirene', 'siret']):
                establishment_data.append(ds)
        
        analysis['data'] = {
            'sources_concurrence': len(establishment_data),
            'datasets_pertinents': [ds.get('title', '')[:60] for ds in establishment_data[:3]]
        }
        
        if establishment_data:
            analysis['insights'].append(f"{len(establishment_data)} sources de donn√©es sur les √©tablissements")
            analysis['insights'].append("Possibilit√© d'identifier les concurrents directs")
            analysis['insights'].append("Analyse SWOT recommand√©e:")
            analysis['insights'].append("  ‚Ä¢ Forces: Votre diff√©renciation")
            analysis['insights'].append("  ‚Ä¢ Faiblesses: √Ä am√©liorer")
            analysis['insights'].append("  ‚Ä¢ Opportunit√©s: Niches non exploit√©es")
            analysis['insights'].append("  ‚Ä¢ Menaces: Concurrents √©tablis")
        else:
            analysis['insights'].append("Peu de donn√©es concurrentielles disponibles")
            analysis['insights'].append("Compl√©tez avec recherche terrain locale")
        
        return analysis
    
    def _analyze_step4_positioning(self, datasets, sector, location):
        """√âTAPE 4: D√©finir le positionnement"""
        analysis = {
            'title': '√âtape 4 - Positionnement strat√©gique',
            'data': {},
            'insights': []
        }
        
        # Strat√©gies de positionnement par secteur
        positioning = {
            'restaurant': {
                'strategies': ['Qualit√© premium', 'Rapidit√©/Prix bas', 'Cuisine sp√©cialis√©e', 'Bio/Local'],
                'differentiation': ['Menu unique', 'Ambiance', 'Service', 'Origine produits']
            },
            'boulangerie': {
                'strategies': ['Artisanat traditionnel', 'Innovation', 'Bio/Sans gluten', 'Prix comp√©titifs'],
                'differentiation': ['Recettes uniques', 'Horaires √©tendus', 'Produits locaux', 'Service personnalis√©']
            },
            'commerce': {
                'strategies': ['Sp√©cialisation', 'Diversit√©', 'Prix', 'Service client'],
                'differentiation': ['Expertise', 'Gamme unique', 'Conseil', 'Exp√©rience']
            },
            'technologie': {
                'strategies': ['Innovation', 'Open-source', 'IA/Automation', 'Sur-mesure'],
                'differentiation': ['Technologie unique', 'Support', 'Prix', 'Rapidit√©']
            }
        }
        
        sector_lower = sector.lower()
        strategy_data = None
        
        for key, value in positioning.items():
            if key in sector_lower or sector_lower in key:
                strategy_data = value
                break
        
        if not strategy_data:
            strategy_data = {
                'strategies': ['Qualit√©', 'Prix', 'Service', 'Innovation'],
                'differentiation': ['Expertise', 'Proximit√©', 'Personnalisation', 'Rapidit√©']
            }
        
        analysis['data'] = {
            'strategies_possibles': strategy_data['strategies'],
            'axes_differentiation': strategy_data['differentiation']
        }
        
        analysis['insights'].append("Strat√©gies de positionnement recommand√©es:")
        for i, strat in enumerate(strategy_data['strategies'], 1):
            analysis['insights'].append(f"  {i}. {strat}")
        
        analysis['insights'].append("\nAxes de diff√©renciation possibles:")
        for diff in strategy_data['differentiation']:
            analysis['insights'].append(f"  ‚Ä¢ {diff}")
        
        if location:
            analysis['insights'].append(f"\nAdaptez votre positionnement au contexte de {location}")
        
        return analysis
    
    def _analyze_step5_business_plan(self, datasets, sector, location):
        """√âTAPE 5: √âlaborer le business plan"""
        analysis = {
            'title': '√âtape 5 - Business Plan',
            'data': {},
            'insights': []
        }
        
        # Donn√©es √©conomiques disponibles
        economic_data = []
        for ds in datasets:
            title_lower = ds.get('title', '').lower()
            if any(word in title_lower for word in ['√©conomique', 'emploi', 'd√©mographique', 'population', 'revenus']):
                economic_data.append(ds)
        
        analysis['data'] = {
            'sources_economiques': len(economic_data),
            'datasets_pour_bp': [ds.get('title', '')[:60] for ds in economic_data[:3]]
        }
        
        analysis['insights'].append("√âl√©ments du Business Plan:")
        analysis['insights'].append("\nInvestissement initial:")
        analysis['insights'].append("  ‚Ä¢ Local/Loyer")
        analysis['insights'].append("  ‚Ä¢ √âquipement/Mat√©riel")
        analysis['insights'].append("  ‚Ä¢ Stock initial")
        analysis['insights'].append("  ‚Ä¢ Marketing/Communication")
        
        analysis['insights'].append("\nPr√©visions financi√®res:")
        analysis['insights'].append("  ‚Ä¢ Chiffre d'affaires pr√©visionnel")
        analysis['insights'].append("  ‚Ä¢ Charges fixes et variables")
        analysis['insights'].append("  ‚Ä¢ Seuil de rentabilit√©")
        analysis['insights'].append("  ‚Ä¢ Cash-flow sur 3 ans")
        
        if economic_data:
            analysis['insights'].append(f"\n{len(economic_data)} sources de donn√©es √©conomiques disponibles")
            analysis['insights'].append("Utilisez ces donn√©es pour affiner vos pr√©visions")
        else:
            analysis['insights'].append("\nCompl√©tez avec donn√©es chambre de commerce locale")
        
        analysis['insights'].append("\nProchaines actions:")
        analysis['insights'].append("  1. T√©l√©charger les datasets pertinents")
        analysis['insights'].append("  2. Analyser la d√©mographie locale")
        analysis['insights'].append("  3. Calculer le march√© potentiel")
        analysis['insights'].append("  4. √âtablir les projections financi√®res")
        analysis['insights'].append("  5. R√©diger le plan d'action")
        
        return analysis
    
    def analyze_market(self, sector, location=None):
        """
        Analyser un march√© sp√©cifique
        
        Args:
            sector: Secteur d'activit√© (ex: "commerce", "restauration", "technologie")
            location: Zone g√©ographique (ex: "Nice", "Marseille", "PACA")
        
        Returns:
            dict: R√©sultats de l'analyse
        """
        analysis = {
            'sector': sector,
            'location': location,
            'timestamp': datetime.now().isoformat(),
            'datasets_found': [],
            'ai_analysis': None,
            'ai_recommendations': None,
            'competition_analysis': None,
            'key_indicators': {},
            'competitors': [],
            'recommendations': [],
            'ollama_enabled': self.ai_analyzer.ollama_available,
            'search_terms_used': []
        }
        
        # Obtenir les termes de recherche √©largis
        search_terms = self._expand_search_terms(sector)
        analysis['search_terms_used'] = search_terms
        
        print(f"üîç Recherche avec les termes: {', '.join(search_terms[:3])}")
        
        # Essayer plusieurs recherches pour maximiser les r√©sultats - EN PARALL√àLE
        all_datasets = []
        max_terms = 2  # R√©duire √† 2 termes maximum pour la vitesse
        
        for term in search_terms[:max_terms]:  # Limiter √† 2 termes pour la vitesse
            query = term
            if location:
                query += f" {location}"
            
            datasets = self.data_client.search_datasets(query, page_size=8)  # R√©duire √† 8 r√©sultats
            
            if datasets and 'data' in datasets:
                all_datasets.extend(datasets['data'])
                
            # Si on a d√©j√† 15+ datasets, arr√™ter la recherche
            if len(all_datasets) >= 15:
                break
        
        # D√©dupliquer par ID
        seen_ids = set()
        unique_datasets = []
        for ds in all_datasets:
            ds_id = ds.get('id')
            if ds_id and ds_id not in seen_ids:
                seen_ids.add(ds_id)
                unique_datasets.append(ds)
        
        print(f"{len(unique_datasets)} datasets uniques trouv√©s")
        
        if unique_datasets:
            analysis['datasets_found'] = [
                {
                    'title': ds.get('title', 'Sans titre'),
                    'description': ds.get('description', '')[:150] + '...' if ds.get('description') else '',
                    'url': ds.get('page', ''),
                    'organization': ds.get('organization', {}).get('name', 'Inconnu') if ds.get('organization') else 'Inconnu',
                    'resources_count': len(ds.get('resources', []))
                }
                for ds in unique_datasets[:8]
            ]
            
            # ‚ú® EX√âCUTER LES 5 √âTAPES D'ANALYSE DE MARCH√â
            print("Ex√©cution des 5 √©tapes d'analyse de march√©...")
            
            analysis['market_steps'] = {
                'step1': self._analyze_step1_market_size(unique_datasets, sector, location),
                'step2': self._analyze_step2_target_audience(unique_datasets, sector, location),
                'step3': self._analyze_step3_competition(unique_datasets, sector, location),
                'step4': self._analyze_step4_positioning(unique_datasets, sector, location),
                'step5': self._analyze_step5_business_plan(unique_datasets, sector, location)
            }
            
            # Analyse IA des datasets (seulement si des datasets trouv√©s)
            if self.ai_analyzer.ollama_available and len(unique_datasets) > 0:
                print("Analyse IA rapide en cours avec Ollama...")
                
                # Analyse des datasets (prioritaire) - SEULEMENT 2 datasets pour plus de vitesse
                analysis['ai_analysis'] = self.ai_analyzer.analyze_datasets(
                    unique_datasets[:2], sector, location
                )
                
                # Recommandations (optionnel - seulement si rapide)
                analysis['ai_recommendations'] = self.ai_analyzer.generate_recommendations(
                    sector, location, len(unique_datasets)
                )
                
                # Analyse concurrentielle D√âSACTIV√âE pour vitesse
                # R√©activer si besoin en d√©commentant la ligne ci-dessous
                # analysis['competition_analysis'] = self.ai_analyzer.analyze_competition(sector, location)
                
                print("Analyse IA termin√©e")
        else:
            # Aucun dataset trouv√© - g√©n√©rer des recommandations g√©n√©riques
            print("Aucun dataset trouv√© - g√©n√©ration de recommandations g√©n√©riques")
            analysis['datasets_found'] = []
            
            # Si Ollama disponible, donner des conseils m√™me sans donn√©es
            if self.ai_analyzer.ollama_available:
                analysis['ai_recommendations'] = self.ai_analyzer.generate_generic_recommendations(sector, location)
        
        # G√©n√©rer des recommandations basiques (fallback)
        analysis['recommendations'] = self._generate_recommendations(sector, location)
        
        return analysis
    
    def _generate_recommendations(self, sector, location):
        """G√©n√©rer des recommandations pour l'√©tude de march√©"""
        recommendations = [
            {
                'category': 'D√©finition du march√©',
                'text': f"Analyser le secteur {sector}" + (f" dans la zone {location}" if location else ""),
                'priority': 'high'
            },
            {
                'category': 'Analyse de la demande',
                'text': "√âtudier les comportements d'achat et cr√©er des personas clients",
                'priority': 'high'
            },
            {
                'category': 'Analyse de l\'offre',
                'text': "Identifier les concurrents directs et indirects via les donn√©es publiques",
                'priority': 'medium'
            },
            {
                'category': 'Conformit√© RGPD',
                'text': "Toutes les donn√©es utilis√©es sont publiques et conformes au RGPD",
                'priority': 'info'
            }
        ]
        return recommendations
    
    def get_economic_indicators(self, location):
        """R√©cup√©rer des indicateurs √©conomiques pour une zone"""
        query = f"√©conomie {location} entreprises"
        datasets = self.data_client.search_datasets(query, page_size=10)
        
        indicators = {
            'location': location,
            'datasets_available': 0,
            'sources': []
        }
        
        if datasets and 'data' in datasets:
            indicators['datasets_available'] = len(datasets['data'])
            indicators['sources'] = [
                {
                    'title': ds.get('title', ''),
                    'url': ds.get('page', '')
                }
                for ds in datasets['data'][:3]
            ]
        
        return indicators


# Initialiser l'analyseur
analyzer = MarketAnalyzer()


@app.route('/')
def index():
    """Page d'accueil"""
    return render_template('index.html')


@app.route('/api/analyze', methods=['POST'])
def analyze():
    """Endpoint pour lancer une analyse de march√©"""
    data = request.get_json()
    
    sector = data.get('sector', '')
    location = data.get('location', '')
    
    if not sector:
        return jsonify({'error': 'Le secteur est requis'}), 400
    
    # Lancer l'analyse
    result = analyzer.analyze_market(sector, location)
    
    return jsonify(result)


@app.route('/api/datasets/search', methods=['GET'])
def search_datasets():
    """Rechercher des datasets"""
    query = request.args.get('q', '')
    
    if not query:
        return jsonify({'error': 'Param√®tre de recherche requis'}), 400
    
    client = DataGouvClient()
    results = client.search_datasets(query)
    
    return jsonify(results)


@app.route('/api/indicators/<location>', methods=['GET'])
def get_indicators(location):
    """R√©cup√©rer les indicateurs √©conomiques d'une zone"""
    indicators = analyzer.get_economic_indicators(location)
    return jsonify(indicators)


@app.route('/api/health', methods=['GET'])
def health():
    """V√©rifier l'√©tat de l'API"""
    ollama_status = 'available'
    ollama_model = None
    
    try:
        models = ollama.list()
        ollama_model = OLLAMA_MODEL
        if not any(m['name'].startswith(OLLAMA_MODEL) for m in models.get('models', [])):
            ollama_status = 'model_not_found'
    except Exception as e:
        ollama_status = 'unavailable'
    
    return jsonify({
        'status': 'ok',
        'timestamp': datetime.now().isoformat(),
        'data_gouv_api': DATA_GOUV_API,
        'ollama_status': ollama_status,
        'ollama_model': ollama_model
    })


@app.route('/api/ollama/models', methods=['GET'])
def list_models():
    """Lister les mod√®les Ollama disponibles"""
    try:
        models = ollama.list()
        return jsonify({
            'available': True,
            'models': [m['name'] for m in models.get('models', [])],
            'current_model': OLLAMA_MODEL
        })
    except Exception as e:
        return jsonify({
            'available': False,
            'error': str(e)
        }), 503


if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
